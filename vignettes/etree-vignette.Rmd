---
title: "etree: Classification and Regression With Structured and Mixed-Type Data"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{etree: Classification and Regression With Structured and Mixed-Type Data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```


# Introduction

R package etree


The first thing to do to set up for analysis is loading the package
<tt>etree</tt>.

```{r setup}
library(etree)
```


# Data generation

We need to generate data for our examples. Along this vignette, we focus on
classification and regression tasks. To make things easy, we can use the same
generation process in the two cases and just change the type of the response. We
consider $100$ observations divided into four equal-sized and disjoint groups
$G_1, G_2, G_3, G_4$. The response variable for regression is defined as:

$$Y_i \sim \begin{cases}
      \mathcal{N}(0, 1) & \text{for} \; i \in G_1\\
      \mathcal{N}(1, 1) & \text{for} \; i \in G_2\\
      \mathcal{N}(2, 1) & \text{for} \; i \in G_3\\
      \mathcal{N}(3, 1) & \text{for} \; i \in G_4
    \end{cases}$$

Hence, we create the corresponding R object, which should be of class
<tt>"numeric"</tt> to be correctly recognized by <tt>etree()</tt> as a response
variable for regression.

```{r}
# Set seed 
set.seed(3)

# Number of observation
n_obs <- 100

# Response variable for regression
resp_reg <- c(rnorm(n_obs / 4, mean = 0, sd = 1),
              rnorm(n_obs / 4, mean = 1, sd = 1),
              rnorm(n_obs / 4, mean = 2, sd = 1),
              rnorm(n_obs / 4, mean = 3, sd = 1))
```

The response variable used for classification is a nominal variable measured at
four different levels corresponding to the four groups. It should be defined as
a <tt>factor</tt> to be correctly identified by <tt>etree()</tt>.

```{r}
# Response variable for regression
resp_cls <- factor(rep(c(1, 2, 3, 4),
                       each = n_obs / 4))
```


For both tasks, covariates are defined as follows:

1. Numeric: 
$$X_{1i} \sim \begin{cases} 
\mathcal{U}[0, 0.55] & \text{for} \; i \in G_1 \cup G_2\\
\mathcal{U}[0.45, 1] & \text{for} \; i \in G_3 \cup G_4
\end{cases};$$

2. Nominal: $$X_{2i} \sim \begin{cases} 
\mathcal{Bin}(0.1) & \text{for} \; i \in G_1 \cup G_2\\
\mathcal{Bin}(0.9) & \text{for} \; i \in G_3 \cup G_4
\end{cases};$$

3. Functions: $X_{3i}$ is distributed as a Gaussian random process over $100$
evaluation points from $0$ to $1$, with mean $0$ and the identity matrix as
covariance matrix, for $i \in G_1 \cup G_3$; as a Gaussian random process over
$100$ evaluation points from $0$ to $1$, with mean $1$ and the identity matrix
as covariance matrix, for $i \in G_2 \cup G_4$;

4. Graphs: $X_{4i}$ is an Erdős–Rényi random graph with $100$ vertices and
connection probability equal to $0.1$ for $i \in G_1 \cup G_3$, and an
Erdős–Rényi random graph with $100$ vertices and connection probability equal to
$0.9$ for $i \in G_2 \cup G_4$.

The set of covariates can be generated by recalling the requirements of
<tt>etree()</tt>: it has to be a list, where each element is a different
variable; numeric variables should be numeric vectors, nominal variable should
be factors, functional variables should be objects of class <tt>"fdata"</tt>,
and variables in the forms of graphs should be lists of objects having class
<tt>"igraph"</tt>.

```{r}
# Numeric
x1 <- c(runif(n_obs / 2, min = 0, max = 0.55),
        runif(n_obs / 2, min = 0.45, max = 1))

# Nominal
x2 <- factor(c(rbinom(n_obs / 2, 1, 0.1),
               rbinom(n_obs / 2, 1, 0.9)))

# Functions
x3 <- c(fda.usc::rproc2fdata(n_obs / 4, seq(0, 1, len = 100), sigma = 1),
        fda.usc::rproc2fdata(n_obs / 4, seq(0, 1, len = 100), sigma = 1,
                             mu = rep(1, 100)),
        fda.usc::rproc2fdata(n_obs / 4, seq(0, 1, len = 100), sigma = 1),
        fda.usc::rproc2fdata(n_obs / 4, seq(0, 1, len = 100), sigma = 1,
                             mu = rep(1, 100)))

# Graphs
x4 <- c(lapply(1:(n_obs / 4), function(j) igraph::sample_gnp(100, 0.1)),
        lapply(1:(n_obs / 4), function(j) igraph::sample_gnp(100, 0.9)),
        lapply(1:(n_obs / 4), function(j) igraph::sample_gnp(100, 0.1)),
        lapply(1:(n_obs / 4), function(j) igraph::sample_gnp(100, 0.9)))

# Covariates list
cov_list <- list(X1 = x1, X2 = x2, X3 = x3, X4 = x4)

```



# Regression

The first example is regression. The set of covariates is <tt>cov_list</tt> and
the response variable is <tt>resp_reg</tt>. First, we perform a "quick fit",
taking advantage of <tt>etree()</tt>'s default parameters. Then, we see the
effects of changing the main ones, and we do so by inspecting the corresponding
plots. Finally, we use a fitted Energy Trees to explore other useful
functionalities such as printing and predicting.

## Quick fit

A quick fit can be simply obtained using <tt>etree()</tt> and specifying the set
of covariates and the response variable only.

```{r}
etree_fit <- etree(response = resp_reg,
                   covariates = cov_list)
```

Let's plot the fitted tree, which is as simple as calling <tt>plot()</tt> on 
<tt>etree_fit</tt>.

```{r, fig.dim = c(9, 5)}
plot(etree_fit)
```

This is a good place to stop and comment the main features of <tt>etree</tt>'s
plots. Many of them descend from the <tt>partykit</tt> version: 

* the number in the square box is the id of the node;
* the text below the square box is the name of the variable selected for
splitting;
* the number right below is the p-value for the Energy test of
independence between the  splitting covariate and the response variable;
* when the splitting covariate is traditional, values on the edges represent the
split point for that variable;
* terminal nodes have a header showing the id and the size of the node; 
* terminal nodes contain different plot types, which depend on the nature of the
response: bar plots for classification, box plots for regression.

In the <tt>etree</tt> version, some novelties have been introduced:

* inner nodes are colored differently to distinguish the various types of
covariates: salmon for numeric variables, light blue for nominal variables,
yellow for functions, light green for variables in the form of graphs;
<!-- "khaki1", "darkseagreen1", "aliceblue", "lightsalmon" -->
* when the splitting covariate is structured and <tt>split_type = "coeff"</tt>,
the text below the square box is the name of the splitting covariate followed by
a dot and the id of the selected component;
* when the splitting covariate is structured, values on the edges represent the
split point with respect to the selected component of the splitting covariate
when <tt>split_type = "coeff"</tt>, or alternatively the size of the
corresponding kid node when <tt>split_type = "cluster"</tt>.

Descending into the detail of the previous fit, we have five terminal nodes as
opposed to four. This gives us the chance to explore some of the optional
parameters to see how they work and possibly to get the expected result.


## Changing parameters

Inspecting the latest plot, we suspect that node $4$ should not be split. In
other words, the tree should be pruned. We can achieve this either by reducing
the parameter <tt>alpha</tt> or by increasing the parameter <tt>minbucket</tt>.
We go for the latter, choosing the rule-of-thumb value of $10\%$ of the sample
size.


```{r}
etree_fit1 <- etree(response = resp_reg,
                    covariates = cov_list,
                    minbucket = 10)
```

Let's take a look at its plot.

```{r, fig.dim = c(7.5, 5)}
plot(etree_fit1)
```

...et voilà! Four terminal nodes, as expected. Increasing <tt>minbucket</tt> has
allowed us to avoid the unnecessary split of node $4$.

It could be difficult to obtain the same result reducing <tt>alpha</tt>,
especially noticing how small the p-values in the first plot are. <!-- with
alpha=0.03, you actually avoid that split, but you also delete node 7's -->
Instead, we can increase it to see how the tree is affected.

```{r}
etree_fit2 <- etree(response = resp_reg,
                    covariates = cov_list,
                    alpha = 1)
```

```{r, fig.dim = c(16, 7)}
plot(etree_fit2)
```

We obtain a much larger tree with $9$ terminal nodes. Splits are made with
respect to all four variables. Consequently, we can notice the full spectrum of
colors for covariates that are accepted by <tt>etree()</tt>.

Another important argument is <tt>split_type</tt>, which defines the splitting
method. By default it is equal to <tt>"coeff"</tt>, so we can change it to
<tt>"cluster"</tt> to see what happens.


```{r}
etree_fit3 <- etree(response = resp_reg,
                    covariates = cov_list,
                    split_type = 'cluster')
```

```{r, fig.dim = c(7.5, 5)}
plot(etree_fit3)
```

In this case, we obtain a tree that is structurally equivalent and has the same
hierarchy among covariates as the one produced with <tt>split_type =
"coeff"</tt>. The tiny differences in the the terminal nodes' size are due to
the different splitting method for covariates $X_3$ and $X_4$.

The plot allows visualizing two differences that are specific to the case of
<tt>split_type = "cluster"</tt>: the text below the square box is always the 
name of the splitting covariate, regardless of its nature; when the splitting
covariate is structured, values on the edges represent the corresponding kid
node's size.


## Print and other utilities

Fitted trees can be printed by using <tt>print()</tt>. The output produced for
the fit with four terminal nodes is the following.

```{r}
print(etree_fit1)
```

Generally speaking, the first part of the output is given by the <tt>Model
formula</tt>. The second one is the textual tree structure provided in
<tt>Fitted party</tt>. The ids of the nodes are between square brackets, and
they are followed, with the only exception of the root, by the name of splitting
covariate (with the addition of the id of the selected component if the
covariate is structured and <tt>split_type = "coeff"</tt>). What follows and
ends before the semicolon is the information on the split that is provided on
the edges in plots. After that, the estimated value of the response in the
terminal node is provided together with the node size, and an error for the
estimate (weighted MSE for regression, weighted misclassification error for
classification). At the end of the output, both the number of inner and terminal
nodes is returned.

In the specific case, there are three inner nodes and four terminal nodes, with
the latter correctly reflecting the division into groups. Estimated values for
the response in the four terminal nodes are quite close to the theoretical
values $0$, $1$, $2$ and $3$.

The fitted tree can be subset using the <tt>[</tt> and <tt>[[</tt> operators,
which yield equivalent results.

```{r}
print(etree_fit1[2])
```

```{r}
print(etree_fit1[[2]])
```

These subtrees can be also plotted.

```{r, fig.dim = c(7.5, 5)}
plot(etree_fit1[2])
```

Other useful functions are <tt>nodeids()</tt> and <tt>nodeapply()</tt>, which
are kept unchanged from their <tt>partykit</tt> version. The first one allows
retrieving the ids of the nodes. By default, it returns all the ids.

```{r}
nodeids(etree_fit1)
```

It can also be used to obtain only the terminal nodes' ids.

```{r}
nodeids(etree_fit1, terminal = TRUE)
```

Finally, one can get the ids of the nodes that belong to a subtree starting from
a given node.

```{r}
nodeids(etree_fit1, from = 2)
```

Function <tt>nodeapply()</tt> applies a function <tt>FUN</tt> to the nodes
specified via the <tt>ids</tt> argument. This can be useful for extracting the
<tt>info</tt> possibly contained in the nodes, such as the pvalues for the
splitting covariate in inner nodes.

```{r}
# Select inner nodes' ids
tnodes <- nodeids(etree_fit1, terminal = TRUE)
nodes <- 1:max(tnodes)
inodes <- nodes[-tnodes]

# Retrieve pvalues
nodeapply(etree_fit1, ids = inodes, FUN = function(n) n$info$pvalue)
```

Finally, the dimensions of the tree can be inspected and retrieved using three
functions giving the total number of nodes (<tt>length()</tt>), the depth of the
tree (<tt>depth()</tt>), and the width of the tree (<tt>width()</tt>).

```{r}
length(etree_fit1)
depth(etree_fit1)
width(etree_fit1)
```


## Predict
<!-- sum((resp_reg[1:25] - mean(resp_reg[1:25]))^2) -->
<!-- sum((predict(etree_fit)[1:25] - mean(predict(etree_fit)[1:25]))^2) -->


# Classification 

## Quick fit

```{r}
etree_fit <- etree(response = resp_cls,
                   covariates = cov_list)
```

```{r, fig.dim = c(7.5, 5)}
plot(etree_fit)
```
<!-- comment on the plot differences with regression -->

## Print


## Predict






# Energy Forests